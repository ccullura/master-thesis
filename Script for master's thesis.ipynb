{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afe8f94a",
   "metadata": {},
   "source": [
    "# Script for master's thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ead26",
   "metadata": {},
   "source": [
    "This script contains the code for all the models included in the thesis. \n",
    "\n",
    "The script consist of the following: \n",
    "- Loading all the packages\n",
    "- Importing data\n",
    "- Defining ROC plot\n",
    "- Logistsic regression \n",
    "    - Standard logistic regression\n",
    "    - Ridge regression\n",
    "    - Lasso regression\n",
    "    - ROC plot for the three regression models\n",
    "- Random forest\n",
    "    - Variable importance\n",
    "- Neural network\n",
    "    - Definding the two neural networks\n",
    "    - Early stopping\n",
    "    - Neural network model 1\n",
    "    - Neural network model 2\n",
    "    - ROC plot for both neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476ae8c4",
   "metadata": {},
   "source": [
    "## Loading all the packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a2ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from matplotlib import animation, cm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy import stats, optimize\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "seed_number = 1000\n",
    "print_flag = 1\n",
    "plot_flag = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3304b51",
   "metadata": {},
   "source": [
    "## Importing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfd1a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "df=pd.read_excel(r'C:\\Users\\cullu\\OneDrive - CBS - Copenhagen Business School\\Speciale\\Output_final.xlsx')\n",
    "print(df)\n",
    "\n",
    "# Removing the columns not needed from the dataset\n",
    "useless = [\"recordID\",\"start_date\", \"end_date\", \"postal_code\",\"premium\",\"value\",\"brand\", \"age_group\", \"seniority_group\", \"hp_group\", \"fuel_group\"]\n",
    "df_without_cats = df.drop(useless, axis=1)\n",
    "\n",
    "# Adding dummies for 'age', 'seniority', 'hp', and 'fuel'\n",
    "dummy_df=pd.get_dummies(df_without_cats,columns=[\"age\",\"seniority\",\"hp\", \"fuel\"])\n",
    "#print(dummy_df)\n",
    "\n",
    "\n",
    "# Splitting dataset into two sets, X for the predictors and y for the response variable                                \n",
    "X = dummy_df.drop([\"status\"], axis = 1)\n",
    "y = dummy_df[\"status\"]\n",
    "\n",
    "# Splitting X and y into training dataset and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb974f7",
   "metadata": {},
   "source": [
    "## Defining ROC plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65822c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_plot(values, colors, h=10, w=7, lab=None):\n",
    "    sns.set()\n",
    "    fig, ax = plt.subplots(figsize=(h, w))\n",
    "    \n",
    "    colors = colors\n",
    "    i=0\n",
    "    for val in values:\n",
    "        ax.plot(val[1], val[0], label = '{} - AUC = {}'.format(lab[i], val[2]), color=colors[i])\n",
    "        i += 1\n",
    "        \n",
    "    ax.plot(np.linspace(0, 1, 30), np.linspace(0, 1, 30), label='baseline', linestyle='--', color='orange')\n",
    "    plt.title('Receiver Operating Characteristic', fontsize=18)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend(loc = 'lower right', fontsize=12)\n",
    "    \n",
    "    return values, colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c2f24a",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4500a53",
   "metadata": {},
   "source": [
    "For logistsic regression we have computed three models: \n",
    "1. Standard logistic regression\n",
    "2. Ridge regression\n",
    "3. Lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f754bf97",
   "metadata": {},
   "source": [
    "### Standard logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237e0722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a standard logistic regression model\n",
    "LR_stand = LogisticRegression()\n",
    "\n",
    "# Gridsearch is used to find the optimal hyperparameters\n",
    "parameters = {\n",
    "    'solver': ['newton-cg', 'lbfgs', 'sag' and 'saga'],\n",
    "    'penalty': ['none']\n",
    "            }\n",
    "LR_stand_Grid = GridSearchCV(                 LR_stand, \n",
    "                                         parameters, \n",
    "                                         cv=5, \n",
    "                                         scoring = \"roc_auc\", \n",
    "                                         # verbose=10, \n",
    "                                         n_jobs = -1\n",
    "                                         )\n",
    "\n",
    "# Training the standard logistic model\n",
    "LR_stand_Grid.fit(X_train, y_train)\n",
    "print(sorted(LR_stand_Grid.cv_results_.keys()))\n",
    "LR_stand = LR_stand_Grid\n",
    "\n",
    "# Printing the optimal hyperparameters\n",
    "print(LR_stand.best_estimator_)\n",
    "    \n",
    "# Model prediction\n",
    "LR_stand_preds = LR_stand.predict(X_test)\n",
    "\n",
    "# Probability predicting for ROC plot\n",
    "LR_stand_probs = LR_stand.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Model prediction with threshold\n",
    "LR_stand_preds = np.where(LR_stand_probs > 0.35, 1, 0)\n",
    "\n",
    "# Creating confusion matrix\n",
    "y_prediction = dict()\n",
    "y_prediction['status'] = y_test.copy()\n",
    "y_prediction[\"Preds\"] = LR_stand_preds\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_prediction['status'], y_prediction[\"Preds\"], rownames=['Actuals'], colnames=['Predicted'])\n",
    "if print_flag == 1: print(\"Confusion Matrix: \\n\",confusion_matrix)\n",
    "\n",
    "# Performance measurements\n",
    "performance_measurements = classification_report(LR_stand_preds, y_test ,output_dict = True)\n",
    "if print_flag == 1: print(\"Performance measurements: \\n\",pd.DataFrame(performance_measurements))\n",
    "\n",
    "# Creating ROC plot\n",
    "if plot_flag == 1:\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, LR_stand_probs)\n",
    "    roc_auc = round(metrics.auc(fpr, tpr), 10)\n",
    "    rates_list = [[tpr, fpr, roc_auc]]\n",
    "    ## Creates the ROC plot\n",
    "    roc_plot(values = rates_list, colors=['blue'], lab = [\"Standard Logistic Regression\"])\n",
    "os.system('say \"Standard logistic regression is done.\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af692dd6",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f0422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a ridge regression model\n",
    "LR_ridge = LogisticRegression()\n",
    "\n",
    "# Gridsearch is used to find the optimal hyperparameters\n",
    "parameters = {\n",
    "    'C': np.logspace(-3,3,50),\n",
    "    'solver': ['newton-cg', 'lbfgs', 'sag' and 'saga'],\n",
    "    'penalty': ['l2']\n",
    "            }\n",
    "LR_ridge_Grid = GridSearchCV(            LR_ridge, \n",
    "                                         parameters, \n",
    "                                         cv=5, \n",
    "                                         scoring = \"roc_auc\", \n",
    "                                         # verbose=10, \n",
    "                                         n_jobs = -1\n",
    "                                         )\n",
    "\n",
    "# Training the standard logistic model\n",
    "LR_ridge_Grid.fit(X_train, y_train)\n",
    "print(sorted(LR_ridge_Grid.cv_results_.keys()))\n",
    "LR_ridge = LR_ridge_Grid\n",
    "\n",
    "# Printing the optimal hyperparameters\n",
    "print(LR_ridge.best_estimator_)\n",
    "    \n",
    "# Model prediction\n",
    "LR_ridge_preds = LR_ridge.predict(X_test)\n",
    "\n",
    "# Probability predicting for ROC plot\n",
    "LR_ridge_probs = LR_ridge.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Model prediction with threshold\n",
    "LR_ridge_preds = np.where(LR_ridge_probs > 0.35, 1, 0)\n",
    "\n",
    "# Creating confusion matrix\n",
    "y_prediction = dict()\n",
    "y_prediction['status'] = y_test.copy()\n",
    "y_prediction[\"Preds\"] = LR_ridge_preds\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_prediction['status'], y_prediction[\"Preds\"], rownames=['Actuals'], colnames=['Predicted'])\n",
    "if print_flag == 1: print(\"Confusion Matrix: \\n\",confusion_matrix)\n",
    "\n",
    "# Performance measurements\n",
    "performance_measurements = classification_report(LR_ridge_preds, y_test, output_dict = True)\n",
    "if print_flag == 1: print(\"Performance measurements: \\n\",pd.DataFrame(performance_measurements))\n",
    "\n",
    "# Creating ROC plot\n",
    "if plot_flag == 1:\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, LR_ridge_probs)\n",
    "    roc_auc = round(metrics.auc(fpr, tpr), 10)\n",
    "    rates_list = [[tpr, fpr, roc_auc]]\n",
    "    ## Creates the ROC plot\n",
    "    roc_plot(values = rates_list, colors=['blue'], lab = [\"Ridge Regression\"])\n",
    "os.system('say \"Ridge regression is done.\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d121e9",
   "metadata": {},
   "source": [
    "### Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c450fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a ridge regression model\n",
    "LR_lasso = LogisticRegression()\n",
    "\n",
    "# Gridsearch is used to find the optimal hyperparameters\n",
    "parameters = {\n",
    "    'C':np.arange(0,10,0.1),\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear','sag' and 'saga'],\n",
    "    'penalty' : ['l1']\n",
    "            }\n",
    "LR_lasso_Grid = GridSearchCV(            LR_lasso, \n",
    "                                         parameters, \n",
    "                                         cv=5, \n",
    "                                         scoring = \"roc_auc\", \n",
    "                                         # verbose=10, \n",
    "                                         n_jobs = -1\n",
    "                                         )\n",
    "\n",
    "# Training the standard logistic model\n",
    "LR_lasso_Grid.fit(X_train, y_train)\n",
    "print(sorted(LR_lasso_Grid.cv_results_.keys()))\n",
    "LR_lasso = LR_lasso_Grid\n",
    "\n",
    "# Printing the optimal hyperparameters\n",
    "print(LR_lasso.best_estimator_)\n",
    "    \n",
    "# Model prediction\n",
    "LR_lasso_preds = LR_lasso.predict(X_test)\n",
    "\n",
    "# Probability predicting for ROC plot\n",
    "LR_lasso_probs = LR_lasso.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Model prediction with threshold\n",
    "LR_lasso_preds = np.where(LR_lasso_probs > 0.35, 1, 0)\n",
    "\n",
    "# Creating confusion matrix\n",
    "y_prediction = dict()\n",
    "y_prediction['status'] = y_test.copy()\n",
    "y_prediction[\"Preds\"] = LR_lasso_preds\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_prediction['status'], y_prediction[\"Preds\"], rownames=['Actuals'], colnames=['Predicted'])\n",
    "if print_flag == 1: print(\"Confusion Matrix: \\n\",confusion_matrix)\n",
    "\n",
    "# Performance measurements\n",
    "performance_measurements = classification_report(LR_lasso_preds, y_test, output_dict = True)\n",
    "if print_flag == 1: print(\"Performance measurements: \\n\",pd.DataFrame(performance_measurements))\n",
    "\n",
    "# Creating ROC plot\n",
    "if plot_flag == 1:\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, LR_lasso_probs)\n",
    "    roc_auc = round(metrics.auc(fpr, tpr), 10)\n",
    "    rates_list = [[tpr, fpr, roc_auc]]\n",
    "    ## Creates the ROC plot\n",
    "    roc_plot(values = rates_list, colors=['blue'], lab = [\"Lasso Regression\"])\n",
    "os.system('say \"Lasso regression is done.\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97772310",
   "metadata": {},
   "source": [
    "### ROC plot for the three regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7757cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating fpr and tpr for standard logistic regression\n",
    "fpr_LR_stand, tpr_LR_stand, threshold_LR_stand = metrics.roc_curve(y_test, LR_stand_probs)\n",
    "roc_auc_LR_stand = round(metrics.auc(fpr_LR_stand, tpr_LR_stand), 4)\n",
    "rates_list_LR_stand = [[tpr_LR_stand, fpr_LR_stand, roc_auc_LR_stand]]\n",
    "\n",
    "# Calculating fpr and tpr for ridge regression\n",
    "fpr_LR_ridge, tpr_LR_ridge, threshold_LR_ridge = metrics.roc_curve(y_test, LR_ridge_probs)\n",
    "roc_auc_LR_ridge = round(metrics.auc(fpr_LR_ridge, tpr_LR_ridge), 4)\n",
    "rates_list_LR_ridge = [[tpr_LR_ridge, fpr_LR_ridge, roc_auc_LR_ridge]]\n",
    "\n",
    "# Calculating fpr and tpr for lasso regression\n",
    "fpr_LR_lasso, tpr_LR_lasso, threshold_LR_lasso = metrics.roc_curve(y_test, LR_lasso_probs)\n",
    "roc_auc_LR_lasso = round(metrics.auc(fpr_LR_lasso, tpr_LR_lasso), 4)\n",
    "rates_list_LR_lasso = [[tpr_LR_lasso, fpr_LR_lasso, roc_auc_LR_lasso]]\n",
    "\n",
    "# Creating the ROC plot for standard logistic regression, lasso regression and ridge regression\n",
    "rates_list = np.array([rates_list_LR_stand, rates_list_LR_ridge, rates_list_LR_lasso])\n",
    "rates_list = rates_list.reshape(3,3)\n",
    "colors_list = ['black'] + ['red'] + ['blue']\n",
    "label_list = [\"Standard Logistic Regression\"] + [\"Ridge Regression\"] + [\"Lasso Regression\"]\n",
    "\n",
    "roc_plot(values = rates_list, colors=colors_list, lab = label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8ac226",
   "metadata": {},
   "source": [
    "## Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c23c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating s random forest model\n",
    "RF = RandomForestClassifier(  random_state = seed_number,\n",
    "                                    bootstrap = True,\n",
    "                                    oob_score = True,\n",
    "                                    )\n",
    "\n",
    "# Gridsearch is used to find the optimal hyperparameters (only the optimal hyperparameters are shown in the gridsearch, many different values have been tested)\n",
    "parameters = {\n",
    "     'criterion': ['entropy'],\n",
    "     'max_depth':[10],\n",
    "     'max_features': [8],\n",
    "     'n_estimators': [1200],\n",
    "             }\n",
    "RF_Grid = GridSearchCV(          RF, \n",
    "                                 parameters, \n",
    "                                 cv=5, scoring = \"roc_auc\", \n",
    "                                 verbose=10, \n",
    "                                 # n_jobs = -1\n",
    "                                         )\n",
    "\n",
    "# Training the standard logistic model\n",
    "RF_Grid.fit(X_train, y_train)\n",
    "print(sorted(RF.cv_results_.keys()))\n",
    "RF = RF_Grid\n",
    "\n",
    "# Printing the optimal hyperparameters\n",
    "print(RF.best_estimator_)\n",
    "    \n",
    "# Model prediction\n",
    "RF_preds = RF.predict(X_test)\n",
    "\n",
    "# Probability predicting for ROC plot\n",
    "RF_probs = RF.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Model prediction with threshold\n",
    "RF_preds = np.where(RF_probs > 0.35, 1, 0)\n",
    "\n",
    "# Creating confusion matrix\n",
    "y_prediction = dict()\n",
    "y_prediction['status'] = y_test.copy()\n",
    "y_prediction[\"Preds\"] = RF_preds\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_prediction['status'], y_prediction[\"Preds\"], rownames=['Actuals'], colnames=['Predicted'])\n",
    "if print_flag == 1: print(\"Confusion Matrix: \\n\",confusion_matrix)\n",
    "\n",
    "# Performance measurements\n",
    "performance_measurements = classification_report(RF_preds, y_test, output_dict = True)\n",
    "if print_flag == 1: print(\"Performance measurements: \\n\",pd.DataFrame(performance_measurements))\n",
    "\n",
    "# Creating ROC plot\n",
    "if plot_flag == 1:\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, RF_probs)\n",
    "    roc_auc = round(metrics.auc(fpr, tpr), 10)\n",
    "    rates_list = [[tpr, fpr, roc_auc]]\n",
    "    ## Creates the ROC plot\n",
    "    roc_plot(values = rates_list, colors=['blue'], lab = [\"Random forest\"])\n",
    "os.system('say \"Random forest is done.\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa0641",
   "metadata": {},
   "source": [
    "### Variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1229301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing variable importance for the random forest model with the optimal hyperparameters\n",
    "RF = RandomForestClassifier(  max_depth = 10, \n",
    "                                    random_state = seed_number,\n",
    "                                    bootstrap = True,\n",
    "                                    criterion = 'entropy', \n",
    "                                    max_features = 8,\n",
    "                                    # max_features = \"sqrt\",\n",
    "                                    n_estimators = 1200,\n",
    "                                    oob_score = True\n",
    "                                    #class_weight = \"balanced\"\n",
    "                                )\n",
    "RF.fit(X_train, y_train)\n",
    "\n",
    "RF.feature_importances_\n",
    "\n",
    "RF.oob_score_\n",
    "\n",
    "import numpy as np\n",
    " \n",
    "importances = RF.feature_importances_\n",
    "#\n",
    "# Sort the feature importance in descending order\n",
    "#\n",
    "sorted_indices = np.argsort(importances)[::-1]\n",
    " \n",
    "feat_labels = dummy_df.columns[1:]\n",
    " \n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f + 1, 30,\n",
    "                            feat_labels[sorted_indices[f]],\n",
    "                            importances[sorted_indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d46b58d",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8992193b",
   "metadata": {},
   "source": [
    "### Definding the two neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2433f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines neural network model 1\n",
    "model1 = tf.keras.Sequential()\n",
    "model1.add(tf.keras.layers.Dense(8, input_dim=X_train.shape[1], activation='relu'))\n",
    "model1.add(tf.keras.layers.Dense(6, activation='relu'))\n",
    "model1.add(tf.keras.layers.Dense(4, activation='relu'))\n",
    "model1.add(tf.keras.layers.Dense(2, activation='relu'))\n",
    "model1.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "#Defines neural network model 2\n",
    "model2 = tf.keras.Sequential()\n",
    "model2.add(tf.keras.layers.Dense(20, input_dim=X_train.shape[1], activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2f095d",
   "metadata": {},
   "source": [
    "### Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa51067",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiles the two models\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "mc1 = tf.keras.callbacks.ModelCheckpoint('model1.h5', monitor='loss', mode='min', verbose=1, save_best_only=True)\n",
    "mc2 = tf.keras.callbacks.ModelCheckpoint('model2.h5', monitor='loss', mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "#transposes the y_train set\n",
    "y_train = np.array(y_train)\n",
    "y_train = y_train.T\n",
    "\n",
    "#defines early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "\n",
    "#make early stopping for neural network model 1\n",
    "EarlyStopping_model_1 = model1.fit(X_train, y_train, epochs=1000, batch_size=10, callbacks=[es, mc1], validation_split=0.2)\n",
    "\n",
    "#print the training loss and validation loss plot for neural network model 1\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "pyplot.plot(EarlyStopping_model_1.history['loss'], label='train')\n",
    "pyplot.plot(EarlyStopping_model_1.history['val_loss'], label='validation error')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "#make early stopping for neural network model 2\n",
    "EarlyStopping_model_2 = model2.fit(X_train, y_train, epochs=1000, batch_size=10, callbacks=[es, mc2], validation_split=0.2)\n",
    "\n",
    "#print the training loss and validation loss plot for neural network model 2\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "pyplot.plot(EarlyStopping_model_2.history['loss'], label='train')\n",
    "pyplot.plot(EarlyStopping_model_2.history['val_loss'], label='validation error')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962d3334",
   "metadata": {},
   "source": [
    "### Neural network model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672d528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have now find the optimal number of epochs and can now set up the two models with optimal parameter:\n",
    "\n",
    "#Neural network model 1\n",
    "def create_NN_model1():\n",
    "    n_weight = 1\n",
    "    end_nodes = 4\n",
    "    NN_model1 = tf.keras.Sequential()\n",
    "    NN_model1.add(Dense(8, activation='relu', input_shape=(24,)))\n",
    "    NN_model1.add(Dense(6, activation='relu'))\n",
    "    NN_model1.add(Dense(4, activation='relu'))\n",
    "    NN_model1.add(Dense(2, activation='relu'))\n",
    "    NN_model1.add(Dense(1, activation='sigmoid'))\n",
    "    #activation = sigmoid for the output layer as we have a binary classification problem \n",
    "\n",
    "    #defines the model loss function, opimizer og goal of perfomance\n",
    "    sgd = SGD(lr=0.01, momentum=0.9)\n",
    "    NN_model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.AUC()])\n",
    "    return NN_model1\n",
    "\n",
    "#Splits the dataset into X set with the predictors and y sets with the response variable                               \n",
    "X = dummy_df.drop([\"status\"], axis = 1)\n",
    "y = dummy_df[\"status\"]\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "\n",
    "scaler = StandardScaler() \n",
    "\n",
    "#Make the test and train datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, encoded_Y, test_size=0.2, random_state=1000)\n",
    "\n",
    "\n",
    "#run the model with the optimal hyperparameters found\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_NN_model1, epochs=150, batch_size=10, verbose=10)))\n",
    "NN_model1 = Pipeline(estimators)\n",
    "\n",
    "#fits the model\n",
    "NN_model_1 = NN_model1.fit(X_train, y_train)\n",
    "\n",
    "NN_Model_preds1 = NN_model_1.predict(X_test)\n",
    "\n",
    "\n",
    "NN_Model_probs1 = NN_model_1.predict_proba(X_test)[:,1]\n",
    "predictions1 = np.where(FF_Model_probs1 > 0.5, 1, 0)\n",
    "\n",
    "y_prediction = dict()\n",
    "y_prediction['status'] = y_test.copy()\n",
    "y_prediction[\"Preds\"] = NN_Model_preds1\n",
    "\n",
    "print(y_prediction)\n",
    "\n",
    "#Model prediction\n",
    "NN_model_probs1 = NN_model_1.predict_proba(X_test)[:,1]\n",
    "NN_Model_preds1 = np.where(NN_model_probs1 > 0.5, 1, 0)\n",
    "\n",
    "#Used for confusion matrix\n",
    "y_prediction['status'] = y_test.copy()\n",
    "y_prediction[\"Preds\"] = NN_Model_preds1\n",
    "\n",
    "#Make confusion matrix\n",
    "confusion_matrixs = pd.crosstab(y_prediction['Preds'], y_prediction[\"status\"], rownames=['Predicted'], colnames=['Actual'])\n",
    "if print_flag == 1: print(\"Confusion Matrix: \\n\",confusion_matrixs)\n",
    "\n",
    "\n",
    "# Performance measurements\n",
    "performance_measurements = classification_report(y_test, y_prediction[\"Preds\"],output_dict = True)\n",
    "if print_flag == 1: print(\"Performance measurements: \\n\",pd.DataFrame(performance_measurements))\n",
    "\n",
    "    \n",
    "#Probability predicting for ROC plot\n",
    "FF_Model_probs = NN_model_1.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "#ROC plot\n",
    "if plot_flag == 1:\n",
    "    fpr1, tpr1, threshold1 = metrics.roc_curve(y_test, NN_model_probs1)\n",
    "    roc_auc1 = round(metrics.auc(fpr1, tpr1), 10)\n",
    "    rates_list1 = [[tpr1, fpr1, roc_auc1]]\n",
    "    \n",
    "    #Creates the ROC plot\n",
    "    roc_plot(values = rates_list1, colors=['blue'], lab = [\"Neural Network model 1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc96ac1",
   "metadata": {},
   "source": [
    "### Neural network model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31276d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural network model 2\n",
    "\n",
    "def create_NN_model2():\n",
    "    # Sequential bygger modellen\n",
    "    n_weight = 1\n",
    "    end_nodes = 4\n",
    "    NN_model2 = tf.keras.Sequential()\n",
    "    NN_model2.add(Dense(20, activation='relu'))\n",
    "    NN_model2.add(Dropout(0.2))\n",
    "    NN_model2.add(Dense(20, activation='relu'))\n",
    "    NN_model2.add(Dropout(0.2))\n",
    "    NN_model2.add(Dense(20, activation='relu'))\n",
    "    NN_model2.add(Dropout(0.2))\n",
    "    NN_model2.add(Dense(20, activation='relu'))\n",
    "    NN_model2.add(Dropout(0.2))\n",
    "    NN_model2.add(Dense(20, activation='relu'))\n",
    "    NN_model2.add(Dropout(0.2))\n",
    "    NN_model2.add(Dense(20, activation='relu'))\n",
    "    NN_model2.add(Dropout(0.2))\n",
    "    NN_model2.add(Dense(20, activation='relu'))\n",
    "    NN_model2.add(Dropout(0.2))\n",
    "    NN_model2.add(Dense(20, activation='relu'))\n",
    "    NN_model2.add(Dropout(0.2))\n",
    "    NN_model2.add(Dense(20, activation='relu'))\n",
    "    NN_model2.add(Dropout(0.2))\n",
    "    NN_model2.add(Dense(1, activation='sigmoid'))\n",
    "    #activation = sigmoid for the output layer as we have a binary classification problem \n",
    "\n",
    "    #defines the model loss function, opimizer og goal of perfomance\n",
    "    sgd = SGD(lr=0.01, momentum=0.9)\n",
    "    NN_model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.AUC()])\n",
    "  \n",
    "    return NN_model2\n",
    "\n",
    "#Splits the dataset into X set with the predictors and y sets with the response variable                               \n",
    "X = dummy_df.drop([\"status\"], axis = 1)\n",
    "y = dummy_df[\"status\"]\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "\n",
    "scaler = StandardScaler() \n",
    "\n",
    "#Make the test and train datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, encoded_Y, test_size=0.2, random_state=1000)\n",
    "\n",
    "\n",
    "#run the model with the optimal hyperparameters found\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_NN_model2, epochs=133, batch_size=32, verbose=10)))\n",
    "NN_model2 = Pipeline(estimators)\n",
    "\n",
    "\n",
    "#fits the model\n",
    "NN_model_2 = NN_model2.fit(X_train, y_train)\n",
    "\n",
    "NN_Model_preds2 = NN_model_2.predict(X_test)\n",
    "\n",
    "\n",
    "NN_Model_probs2 = NN_model_2.predict_proba(X_test)[:,1]\n",
    "predictions2 = np.where(NN_Model_probs2 > 0.5, 1, 0)\n",
    "\n",
    "y_prediction = dict()\n",
    "y_prediction['status'] = y_test.copy()\n",
    "y_prediction[\"Preds\"] = NN_Model_preds2\n",
    "\n",
    "#Model prediction\n",
    "NN_model_2_probs = NN_model_2.predict_proba(X_test)[:,1]\n",
    "NN_model_2_preds = np.where(NN_model_2_probs > 0.5, 1, 0)\n",
    "\n",
    "\n",
    "#Used for confusion matrix\n",
    "y_prediction['status'] = y_test.copy()\n",
    "y_prediction[\"Preds\"] = NN_model_2_preds\n",
    "\n",
    "#Makes confusion matrix\n",
    "confusion_matrixs = pd.crosstab(y_prediction['Preds'], y_prediction[\"status\"], rownames=['Predicted'], colnames=['Actual'])\n",
    "if print_flag == 1: print(\"Confusion Matrix: \\n\",confusion_matrixs)\n",
    "\n",
    "# Performance measurements\n",
    "performance_measurements = classification_report(y_test, y_prediction[\"Preds\"],output_dict = True)\n",
    "if print_flag == 1: print(\"Performance measurements: \\n\",pd.DataFrame(performance_measurements))\n",
    "    \n",
    "#Probability predicting for ROC plot\n",
    "NN_model_2_probs = NN_model_2.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "#ROC plot\n",
    "if plot_flag == 1:\n",
    "    fpr2, tpr2, threshold2 = metrics.roc_curve(y_test, NN_model_2_probs)\n",
    "    roc_auc2 = round(metrics.auc(fpr2, tpr2), 10)\n",
    "    rates_list2 = [[tpr2, fpr2, roc_auc2]]\n",
    "    \n",
    "    #Creates the ROC plot\n",
    "    roc_plot(values = rates_list2, colors=['blue'], lab = [\"Neural Network model 2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786b3537",
   "metadata": {},
   "source": [
    "### ROC plot for both neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d6a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare the ROC and AUC for the two models\n",
    "\n",
    "#Neural network model 1\n",
    "fpr1, tpr1, threshold1 = metrics.roc_curve(y_test, NN_model_probs1)\n",
    "roc_auc1 = round(metrics.auc(fpr1, tpr1), 4)\n",
    "rates_list1 = [[tpr1, fpr1, roc_auc1]]\n",
    "\n",
    "#Neural network model 2\n",
    "fpr2, tpr2, threshold2 = metrics.roc_curve(y_test, NN_model_2_probs)\n",
    "roc_auc2 = round(metrics.auc(fpr2, tpr2), 4)\n",
    "rates_list2 = [[tpr2, fpr2, roc_auc2]]\n",
    "\n",
    "\n",
    "#The combined ROC plot\n",
    "rates_list_combined = [[tpr1, fpr1, roc_auc1], [tpr2, fpr2, roc_auc2]]\n",
    "colors_list = ['blue'] + ['red']\n",
    "label_list = [\"Neural Network model 1\"] + [\"Neural Network model 2\"]\n",
    "\n",
    "#Creates the ROC plot\n",
    "roc_plot(values = rates_list_combined, colors=colors_list, lab = label_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
